{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Жанровая классификация аудио"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "import tensorflow as tf\n",
    "import glob\n",
    "\n",
    "import json\n",
    "\n",
    "from functools import reduce\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import NuSVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "from scipy.optimize import minimize_scalar\n",
    "\n",
    "from keras.backend import clear_session\n",
    "from keras.callbacks import TensorBoard, ModelCheckpoint, CSVLogger, BackupAndRestore\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from keras.utils import Sequence\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.list_physical_devices()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Фильтрация метаданных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = './data/fma_small'\n",
    "METADATA_DIR = './data/fma_metadata/'\n",
    "\n",
    "mp3_files = glob.glob(DATA_DIR + '/*/*.mp3')\n",
    "mp3_names = list(map(lambda f: np.int64(f.split('/')[-1].split('.')[0]), mp3_files))\n",
    "\n",
    "raw_tracks = pd.read_csv(METADATA_DIR + 'raw_tracks.csv')\n",
    "tracks = raw_tracks[raw_tracks['track_id'].isin(mp3_names)]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Сбор признаков, полученных с помощью `librosa`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df = pd.read_csv(METADATA_DIR + 'features.csv', index_col=0, header=[0, 1, 2])\n",
    "features_df = features_df[features_df.index.isin(mp3_names)]\n",
    "\n",
    "features = np.unique(list(map(lambda x: x[0], list(features_df.columns))))\n",
    "\n",
    "print(f\"Features available: {features}\")\n",
    "print(f\"Total: {len(features)}\")\n",
    "\n",
    "features_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Отбор признаков"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим всю имеющуюся информацию о треках"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracks.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Оценим число непустых значений тегов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracks['tags'].map(lambda x: None if x == '[]' else x).notnull().value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подсчитаем число уникальных тегов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_tags = reduce(lambda tags, l: tags.union(eval(l)), tracks['tags'], set())\n",
    "print(len(unique_tags))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Оставим предположительно полезную информацию из набора данных. Убедимся\n",
    "в её необходимости позже."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_keep = [\n",
    "  'track_id', \"album_id\", \"artist_id\", \"track_duration\", \n",
    "  \"track_genres\", \"track_instrumental\", \"track_interest\", \"track_listens\",\n",
    "]\n",
    "\n",
    "filtered_tracks = tracks[to_keep]\n",
    "filtered_tracks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Преобразуем время в секунды"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def duration_to_int(t):\n",
    "  splitted = t.split(\":\")\n",
    "  \n",
    "  return int(splitted[0]) * 60 + int(splitted[1])\n",
    "\n",
    "filtered_tracks.loc[:,'track_duration'] = filtered_tracks.track_duration.apply(duration_to_int)\n",
    "filtered_tracks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Узнаем количество жанров для треков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genres = filtered_tracks['track_genres'].map(lambda x: json.loads(x.replace(\"'\", \"\\\"\")))\n",
    "genre_ids = genres.map(lambda x: list(map(lambda y: y['genre_id'], x)))\n",
    "genre_ids.map(lambda x: len(x)).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Определим базовые жанры для каждого трека"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_genres = pd.read_csv(METADATA_DIR + 'genres.csv')\n",
    "\n",
    "base_genres = genre_ids.map(lambda x: all_genres[all_genres.genre_id == int(x[0])].iloc[0].top_level)\n",
    "\n",
    "filtered_tracks['track_genres'] = base_genres\n",
    "filtered_tracks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_genres.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получили 8 сбалансированных классов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_corr(df):\n",
    "  corr = df.corr()\n",
    "  cmap = sns.diverging_palette(230, 20, as_cmap=True)\n",
    "  mask = np.triu(np.ones_like(corr, dtype=bool))\n",
    "  sns.heatmap(corr, mask=mask, cmap=cmap)\n",
    "  \n",
    "display_corr(filtered_tracks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Жанр трека очень плохо коррелирует с его длительностью, поэтому исключим\n",
    "этот признак из рассмотрения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_tracks = filtered_tracks.drop('track_duration', axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь добавим значения, предпосчитанные с помощью `librosa`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = features_df.merge(filtered_tracks, how='inner', on='track_id')\n",
    "\n",
    "display_corr(merged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Конечно, признаков слишком много. Из всех возьмем признаки с наибольшей по\n",
    "модулю корреляцией.\n",
    "\n",
    "Для этого отсортируем признаки по степени корреляции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation = merged.corr()\n",
    "\n",
    "genres_corr = correlation['track_genres'].sort_values(key=lambda x: np.abs(x), ascending=False)\n",
    "genres_corr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Изобразим распределение значений корреляции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(genres_corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видно, что наибольшее число признаков имеют почти нулевую корреляцию.\n",
    "В связи с этим выберем наиболее информативные из них"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boundary = 0.2\n",
    "\n",
    "selected = merged[genres_corr[abs(genres_corr) > boundary].reset_index()['index']]\n",
    "selected.set_index('track_id', inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кроме того, удалим сильно коррелирующие друг с другом нецелевые признаки,\n",
    "оставив среди таких пар те, что больше коррелируют с целевым"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = selected.corr()\n",
    "\n",
    "to_be_excluded = set()\n",
    "\n",
    "boundary = 0.9\n",
    "\n",
    "for i in c:\n",
    "  for j in c:\n",
    "    if abs(c[i][j]) > boundary and i != j and i != 'track_genres' and j != 'track_genres':\n",
    "      least_informative = i if c['track_genres'][i] < c['track_genres'][j] else j\n",
    "      to_be_excluded.add(least_informative)\n",
    "      \n",
    "to_be_excluded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected = selected.drop(to_be_excluded, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Перекодируем метки классов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genre_le = LabelEncoder()\n",
    "\n",
    "selected.track_genres = genre_le.fit_transform(selected.track_genres)\n",
    "selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected.columns = selected.columns.map(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in selected.columns:\n",
    "  if column == 'track_genres':\n",
    "    continue\n",
    "  selected[column] = StandardScaler().fit_transform(selected[column].to_numpy().reshape(-1, 1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Убедимся, что `StandardScaler` отработал корректно"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected.describe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разделим данные по принципу `train/test/split`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = selected.drop('track_genres', axis=1).to_numpy()\n",
    "y = selected['track_genres'].to_numpy()\n",
    "\n",
    "test_size = 0.2\n",
    "valid_size = 0.1\n",
    "\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(x, y, test_size=0.2, random_state=69, stratify=y)\n",
    "    \n",
    "X_train, X_valid, y_train, y_valid = \\\n",
    "    train_test_split(X_train, y_train, test_size=valid_size / (1 - test_size),\n",
    "                     random_state=69, stratify=y_train)\n",
    "    \n",
    "n_classes = np.max(y) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label2vec(n_classes):\n",
    "  def inner(label):  \n",
    "    v = np.zeros(n_classes)\n",
    "    v[label] = 1\n",
    "    return v\n",
    "  return inner"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Сравнение моделей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Nearest Neighbours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = np.max(y) + 1\n",
    "list_of_neighbours = list(map(int, range(1, 300, 5)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Опишем функцию, которая будет отображать результаты экспериментов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_score(n, scores, names):\n",
    "    d = {names: n, 'score': scores}\n",
    "    df = pd.DataFrame(d)\n",
    "\n",
    "    sns.set(style='darkgrid')\n",
    "    sns.lineplot(x=names, y='score', data=df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вычислим значения `accuracy` для моделей с разным числом соседей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_n = -1\n",
    "best_score = -1\n",
    "scores = []\n",
    "for n in tqdm(list_of_neighbours):\n",
    "    knn = KNeighborsClassifier(p=1, n_neighbors=n)\n",
    "    knn.fit(X_train, y_train)\n",
    "    y_pred = knn.predict(X_test)\n",
    "    score = knn.score(X_test, y_test)\n",
    "    \n",
    "    if score > best_score:\n",
    "        best_score = score\n",
    "        best_n = n\n",
    "    scores.append(score)\n",
    "\n",
    "plot_score(list_of_neighbours, scores, 'neighbors')\n",
    "print(f'Лучшая модель: {best_n} соседей, точность: {best_score}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Конечно, такой метод оценки качества модели не является надежным, лучше воспользоваться\n",
    "оценкой методом кросс-валидации — `cross_val_score`.\n",
    "\n",
    "В дальнейшем будем использовать именно этот метод."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_n = -1\n",
    "best_score = -1\n",
    "scores = []\n",
    "\n",
    "for n in tqdm(list_of_neighbours):\n",
    "    knn = KNeighborsClassifier(p=2, n_neighbors=n)\n",
    "    \n",
    "    score = cross_val_score(knn, x, y, cv=5).mean()\n",
    "    \n",
    "    if score > best_score:\n",
    "        best_score = score\n",
    "        best_n = n\n",
    "    scores.append(score)\n",
    "\n",
    "plot_score(list_of_neighbours, scores, 'neighbors')\n",
    "print(f'Лучшая модель: {best_n} соседей, точность: {best_score}')\n",
    "\n",
    "models.append((KNeighborsClassifier(p=2, n_neighbors=best_n), 'sklearn'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "for n in tqdm(list_of_neighbours):\n",
    "    knn = KNeighborsClassifier(p=2, n_neighbors=n)\n",
    "    knn.fit(X_train, y_train)\n",
    "    probs = knn.predict_proba(X_test)\n",
    "    \n",
    "    loss = log_loss(y_test, probs)\n",
    "    scores.append(loss)\n",
    "\n",
    "plot_score(list_of_neighbours, scores, 'neighbors')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\nu$-svc"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У $\\nu$-svc есть несколько возможных для использования ядер:\n",
    "\n",
    "- linear\n",
    "- poly\n",
    "- rbf\n",
    "- sigmoid\n",
    "\n",
    "Сравним их между собой"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernels = [\"linear\", \"poly\", \"rbf\", \"sigmoid\"]\n",
    "kernels_scores: dict[str, float] = {}\n",
    "for kernel in tqdm(kernels):\n",
    "    nu_svc = NuSVC(kernel=kernel)\n",
    "    kernels_scores[kernel] = cross_val_score(nu_svc, x, y, cv=5).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_score(kernels, kernels_scores.values(), 'kernels')\n",
    "\n",
    "best_kernel = max(kernels_scores, key=kernels_scores.get)\n",
    "print(f'Лучшее ядро для svc: {best_kernel} c точностью {kernels_scores[best_kernel]}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для ядра _poly_ можно выбирать степень полинома. Посмотрим на точность при\n",
    "разных степенях."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "degrees = range(1, 10)\n",
    "\n",
    "degrees_scores: dict[int, float] = {}\n",
    "for degree in tqdm(degrees):\n",
    "    nu_svc = NuSVC(kernel=\"poly\", degree=degree)\n",
    "    degrees_scores[degree] = cross_val_score(nu_svc, x, y, cv=5).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_score(degrees, degrees_scores.values(), 'degrees')\n",
    "\n",
    "best_degree = max(degrees_scores, key=degrees_scores.get)\n",
    "print(f'Для ядра poly лучшая степень: {best_degree} с точностью {degrees_scores[best_degree]}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "То есть ядро с использованием линейной функции определённо лучше полиномиальной."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим rbf в качестве ядра. Она принимает аргументом $\\nu$. Попробуем\n",
    "максимизировать `cross_val_scrore`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nu_cross_val_score(nu):\n",
    "    nu_svc = NuSVC(kernel=\"rbf\", nu=nu)\n",
    "    return -cross_val_score(nu_svc, x, y, cv=5).mean()\n",
    "\n",
    "\n",
    "res = minimize_scalar(nu_cross_val_score, bounds=(0, 1), options={\"xatol\":0.01})\n",
    "best_nu = res.x\n",
    "best_nu_score = -res.fun\n",
    "\n",
    "print(f'Для ядра rbf лучшее значение nu: {best_nu} с точностью: {best_nu_score}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кажется, это лучший результат, который мы можем получить от svc. Едем дальше."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models.append((NuSVC(nu=best_nu), 'sklearn'))\n",
    "\n",
    "models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Нейронные сети"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Базовая модель"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Возьмем в качестве модели многослойный перцептрон, для борьбы с переобучением\n",
    "воспользуемся слоями `Dropout`. Кроме того, добавим между слоями\n",
    "нормализацию по подвыборке для того, чтобы сгладить процесс обучения.\n",
    "\n",
    "В качестве функции активации выберем `leaky_relu`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dropout, Dense, BatchNormalization\n",
    "\n",
    "clear_session()\n",
    "\n",
    "lr = 0.001\n",
    "\n",
    "model = keras.Sequential()\n",
    "model.add(Input(X_train.shape[1]))\n",
    "\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(.3))\n",
    "\n",
    "model.add(Dense(128, activation='leaky_relu'))\n",
    "model.add(Dropout(.2))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dense(32, activation='leaky_relu'))\n",
    "model.add(Dense(8, activation='softmax'))\n",
    "\n",
    "\n",
    "model.compile(optimizer=keras.optimizers.Adam(learning_rate=lr),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['categorical_accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_y_train = np.array(list(map(label2vec(n_classes), y_train)))\n",
    "nn_y_test = np.array(list(map(label2vec(n_classes), y_test)))\n",
    "nn_y_valid = np.array(list(map(label2vec(n_classes), y_valid)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = input()\n",
    "\n",
    "callbacks = [\n",
    "  TensorBoard(),\n",
    "  ModelCheckpoint(f'{run}/checkpoint/', save_best_only=True, save_weights_only=True, monitor='categorical_accuracy', verbose=1),\n",
    "  CSVLogger(\"logs.csv\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, nn_y_train, validation_data=(X_valid, nn_y_valid), epochs=200, callbacks=callbacks, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(f'{run}/checkpoint/')\n",
    "model.evaluate(X_test, nn_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models.append((model, 'keras'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Сравнение моделей, обученных на мета-данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for entry in models:\n",
    "  model_type = entry[1]\n",
    "  model = entry[0]\n",
    "  if model_type == 'sklearn':\n",
    "    model.fit(X_train, y_train)\n",
    "    degrees_scores = model.score(X_test, y_test)\n",
    "  else:\n",
    "    degrees_scores = model.evaluate(X_test, nn_y_test)\n",
    "  print(f'Результат для {model.__class__.__name__}: {degrees_scores}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Сверточная нейронная сеть"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_audio_by_id(id, sr=22050):\n",
    "  id = f'{id:06}'\n",
    "  subfolder = id[:3]\n",
    "  filename = f'{DATA_DIR}/{subfolder}/{id}.mp3'\n",
    "  return librosa.load(filename, sr=sr)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as path\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "from tempfile import mkdtemp, gettempdir\n",
    "from itertools import islice\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class AudioLoader(Sequence):\n",
    "  data: pd.DataFrame\n",
    "  sr: int = 22050\n",
    "  batch_size: int = 16\n",
    "  n_fft: int = 2048\n",
    "  hop_length: int = 512\n",
    "  suffix: int = None\n",
    "  \n",
    "  x = None\n",
    "  y = None\n",
    "  dtemp = None\n",
    "  \n",
    "  def retrieve_image(spec):\n",
    "    fig = plt.figure(frameon=False)\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.set_position([0, 0, 1, 1])\n",
    "    plt.axis('off')\n",
    "    plt.subplots_adjust(left=0, right=1, top=1, bottom=0)\n",
    "    fig.patch.set_alpha(0)\n",
    "    \n",
    "    librosa.display.specshow(spec, ax=ax)\n",
    "    fig.canvas.draw()\n",
    "    rgba_buf = fig.canvas.buffer_rgba()\n",
    "    (w,h) = fig.canvas.get_width_height()\n",
    "    rgba_arr = np.frombuffer(rgba_buf, dtype=np.uint8).reshape((h, w, 4))[:, :, :3]\n",
    "    plt.close()\n",
    "    \n",
    "    return rgba_arr / 255\n",
    "  \n",
    "  def __restore_state__(self):\n",
    "    try:\n",
    "      with open(f'{self.dtemp}/.progress{self.suffix}', 'r') as f:\n",
    "        i = int(f.read())\n",
    "    except OSError:\n",
    "      i = 0\n",
    "    return i\n",
    "  \n",
    "  def __save_state__(self, i):\n",
    "    path = f'{self.dtemp}/.progress{self.suffix}'\n",
    "    with open(path, 'w') as f:\n",
    "      f.write(str(i))\n",
    "  \n",
    "  def __post_init__(self):\n",
    "    try:\n",
    "      dtemp = glob.glob(f'/{gettempdir()}/genre-research*{self.suffix}')[0]\n",
    "      self.dtemp = dtemp\n",
    "      mode = 'r+'\n",
    "    except:\n",
    "      self.dtemp = mkdtemp(prefix='genre-research', suffix=self.suffix)\n",
    "      mode = 'w+'\n",
    "  \n",
    "    self.x = np.memmap(path.join(self.dtemp, 'x.dat'),\n",
    "                       shape=(8000, 480, 640, 3),\n",
    "                       dtype=np.float32,\n",
    "                       mode=mode)\n",
    "    self.y = np.memmap(path.join(self.dtemp, 'y.dat'),\n",
    "                       shape=(8000, 8),\n",
    "                       dtype=np.float32,\n",
    "                       mode=mode)\n",
    "    \n",
    "    init_state = self.__restore_state__()\n",
    "    i = init_state\n",
    "    for index, row in tqdm(islice(self.data.iterrows(), init_state, None, 1), total=len(self.data), initial=init_state):\n",
    "      if i % 100 == 0:\n",
    "        self.__save_state__(i)\n",
    "      \n",
    "      try:\n",
    "        audio = get_audio_by_id(index)\n",
    "      except Exception:\n",
    "        self.x[i] = np.zeros((480, 640, 3))\n",
    "        self.y[i] = np.zeros(8)\n",
    "        i += 1\n",
    "        continue\n",
    "      audio = np.pad(audio, (0, max(self.sr * 31 - audio.shape[0], 0)))\n",
    "      spec = librosa.power_to_db(\n",
    "        librosa.feature.melspectrogram(y=audio, \n",
    "                                        n_fft=self.n_fft, \n",
    "                                        hop_length=self.hop_length,\n",
    "                                        fmax=8000))\n",
    "      \n",
    "      image = AudioLoader.retrieve_image(spec)\n",
    "      self.x[i] = image\n",
    "      self.y[i] = label2vec(8)(int(row['track_genres']))\n",
    "      i += 1\n",
    "    \n",
    "    self.__save_state__(str(len(self.data)))\n",
    "    p = np.random.permutation(self.data.shape[0])\n",
    "    self.x, self.y = self.x[p], self.y[p]\n",
    "    \n",
    "  def __len__(self):\n",
    "    return self.data.shape[0] // self.batch_size\n",
    "  \n",
    "  def __getitem__(self, index):\n",
    "    x_cur = self.x[self.batch_size * index:self.batch_size * (index + 1)]\n",
    "    y_cur = self.y[self.batch_size * index:self.batch_size * (index + 1)]\n",
    "    \n",
    "    return x_cur, y_cur\n",
    "  \n",
    "  def on_epoch_end(self):\n",
    "    p = np.random.permutation(self.data.shape[0])\n",
    "    self.x, self.y = self.x[p], self.y[p]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.DataFrame(columns=selected.columns)\n",
    "valid = pd.DataFrame(columns=selected.columns)\n",
    "test = pd.DataFrame(columns=selected.columns)\n",
    "\n",
    "train_size = 0.8\n",
    "valid_size = 0.1\n",
    "\n",
    "for i in range(0, 8):\n",
    "  cur = selected[selected['track_genres'] == i]\n",
    "  \n",
    "  n = len(cur)\n",
    "  train = pd.concat([train, cur.iloc[:int(train_size * n)]])\n",
    "  valid = pd.concat([valid, cur.iloc[int(train_size * n):int((train_size + valid_size) * n)]])\n",
    "  test = pd.concat([test, cur.iloc[int((train_size + valid_size) * n):]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = AudioLoader(test, suffix='_test', batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_loader = AudioLoader(valid, suffix='_val', batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = AudioLoader(train, suffix='_train', batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Conv2D, Flatten, MaxPooling2D, BatchNormalization, Dense, Dropout\n",
    "from keras.backend import clear_session\n",
    "from keras.regularizers import L2\n",
    "\n",
    "from keras.applications import ResNet50\n",
    "\n",
    "clear_session()\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "resnet50 = ResNet50(include_top=False, input_shape=(480, 640, 3))\n",
    "\n",
    "resnet50.trainable = False\n",
    "\n",
    "model.add(resnet50)\n",
    "model.add(Conv2D(32, (4, 4), strides=(2, 2),\n",
    "                 activation='relu',\n",
    "                 kernel_regularizer=L2(0.01)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(2, 2))\n",
    "\n",
    "# model.add(Conv2D(16, (8, 8), strides=(4, 4), activation='relu',\n",
    "#                  kernel_regularizer=L2(0.01)))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(MaxPooling2D(2, 2))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(8, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=Adam(learning_rate=0.001),\n",
    "              metrics=['categorical_accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "  ModelCheckpoint(\"model/checkpoint\", monitor='val_categorical_accuracy', \n",
    "                  save_best_only=True, save_weights_only=True),\n",
    "  BackupAndRestore(\"backup\"),\n",
    "  CSVLogger(\"logs.csv\"),\n",
    "  TensorBoard()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train_loader, validation_data=val_loader,\n",
    "          batch_size=batch_size, epochs=150,\n",
    "          callbacks=callbacks, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "vscode": {
   "interpreter": {
    "hash": "e64c9a8690db34c99b15a1f7c3e93a32cbaa4232add629869dab3afba3bd6272"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
