{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Жанровая классификация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.list_physical_devices()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Фильтрация метаданных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "\n",
    "DATA_DIR = './data/fma_small/'\n",
    "METADATA_DIR = './data/fma_metadata/'\n",
    "\n",
    "mp3_files = glob.glob(DATA_DIR + '*/*.mp3')\n",
    "mp3_names = list(map(lambda f: np.int64(f.split('/')[-1].split('.')[0]), mp3_files))\n",
    "\n",
    "raw_tracks = pd.read_csv(METADATA_DIR + 'raw_tracks.csv')\n",
    "tracks = raw_tracks[raw_tracks['track_id'].isin(mp3_names)]\n",
    "\n",
    "tracks"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Сбор признаков, полученных с помощью `librosa`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df = pd.read_csv(METADATA_DIR + 'features.csv', index_col=0, header=[0, 1, 2])\n",
    "features_df = features_df[features_df.index.isin(mp3_names)]\n",
    "\n",
    "features = np.unique(list(map(lambda x: x[0], list(features_df.columns))))\n",
    "\n",
    "print(f\"Features available: {features}\")\n",
    "print(f\"Total: {len(features)}\")\n",
    "\n",
    "features_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Отбор признаков"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим всю имеющуюся информацию о треках"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracks.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Оценим число непустых значений тегов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracks['tags'].map(lambda x: None if x == '[]' else x).notnull().value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подсчитаем число уникальных тегов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "\n",
    "\n",
    "unique_tags = reduce(lambda tags, l: tags.union(eval(l)), tracks['tags'], set())\n",
    "print(len(unique_tags))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Оставим предположительно полезную информацию из набора данных. Убедимся\n",
    "в её необходимости позже."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_keep = [\n",
    "  'track_id', \"album_id\", \"artist_id\", \"track_duration\", \n",
    "  \"track_genres\", \"track_instrumental\", \"track_interest\", \"track_listens\",\n",
    "]\n",
    "\n",
    "filtered_tracks = tracks[to_keep]\n",
    "filtered_tracks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Преобразуем время в секунды"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def duration_to_int(t):\n",
    "  splitted = t.split(\":\")\n",
    "  \n",
    "  return int(splitted[0]) * 60 + int(splitted[1])\n",
    "\n",
    "filtered_tracks.loc[:,'track_duration'] = filtered_tracks.track_duration.apply(duration_to_int)\n",
    "filtered_tracks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Узнаем количество жанров для треков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "genres = filtered_tracks['track_genres'].map(lambda x: json.loads(x.replace(\"'\", \"\\\"\")))\n",
    "genre_ids = genres.map(lambda x: list(map(lambda y: y['genre_id'], x)))\n",
    "genre_ids.map(lambda x: len(x)).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Определим базовые жанры для каждого трека"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_genres = pd.read_csv(METADATA_DIR + 'genres.csv')\n",
    "\n",
    "base_genres = genre_ids.map(lambda x: all_genres[all_genres.genre_id == int(x[0])].iloc[0].top_level)\n",
    "\n",
    "filtered_tracks['track_genres'] = base_genres\n",
    "filtered_tracks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_genres.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получили 8 сбалансированных классов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "def display_corr(df):\n",
    "  corr = df.corr()\n",
    "  cmap = sns.diverging_palette(230, 20, as_cmap=True)\n",
    "  mask = np.triu(np.ones_like(corr, dtype=bool))\n",
    "  sns.heatmap(corr, mask=mask, cmap=cmap)\n",
    "  \n",
    "display_corr(filtered_tracks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Жанр трека очень плохо коррелирует с его длительностью, поэтому исключим\n",
    "этот признак из рассмотрения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_tracks = filtered_tracks.drop('track_duration', axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь добавим значения, предпосчитанные с помощью `librosa`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged = features_df.merge(filtered_tracks, how='inner', on='track_id')\n",
    "\n",
    "display_corr(merged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Конечно, признаков слишком много. Из всех возьмем признаки с наибольшей по\n",
    "модулю корреляцией.\n",
    "\n",
    "Для этого отсортируем признаки по степени корреляции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation = merged.corr()\n",
    "\n",
    "genres_corr = correlation['track_genres'].sort_values(key=lambda x: np.abs(x), ascending=False)\n",
    "genres_corr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Изобразим распределение значений корреляции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(genres_corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видно, что наибольшее число признаков имеют почти нулевую корреляцию.\n",
    "В связи с этим выберем наиболее информативные из них"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boundary = 0.2\n",
    "\n",
    "selected = merged[genres_corr[abs(genres_corr) > boundary].reset_index()['index']]\n",
    "selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = selected.corr()\n",
    "\n",
    "to_be_excluded = set()\n",
    "\n",
    "boundary = 0.9\n",
    "\n",
    "for i in c:\n",
    "  for j in c:\n",
    "    if abs(c[i][j]) > boundary and i != j and i != 'track_genres' and j != 'track_genres':\n",
    "      least_informative = i if c['track_genres'][i] < c['track_genres'][j] else j\n",
    "      to_be_excluded.add(least_informative)\n",
    "      \n",
    "to_be_excluded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in to_be_excluded:\n",
    "  selected.drop(feature, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Перекодируем метки классов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "genre_le = LabelEncoder()\n",
    "\n",
    "selected.track_genres = genre_le.fit_transform(selected.track_genres)\n",
    "selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected.columns = selected.columns.map(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "for column in selected.columns:\n",
    "  if column == 'track_genres':\n",
    "    continue\n",
    "  selected[column] = StandardScaler().fit_transform(selected[column].to_numpy().reshape(-1, 1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Убедимся, что `StandardScaler` отработал корректно"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected.describe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разделим данные по принципу `train/test/split`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x = selected.drop('track_genres', axis=1).to_numpy()\n",
    "y = selected['track_genres'].to_numpy()\n",
    "\n",
    "test_size = 0.2\n",
    "valid_size = 0.1\n",
    "\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(x, y, test_size=0.2, random_state=69)\n",
    "    \n",
    "X_train, X_valid, y_train, y_valid = \\\n",
    "    train_test_split(X_train, y_train, test_size=valid_size / (1 - test_size), \n",
    "                     random_state=69)\n",
    "    \n",
    "n_classes = np.max(y) + 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Nearest Neighbours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = np.max(y) + 1\n",
    "ns = list(map(int, range(1, 300, 5)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Опишем функцию, которая будет отображать результаты экспериментов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def plot_score(n, scores, names):\n",
    "    d = {names: n, 'score': scores}\n",
    "    df = pd.DataFrame(d)\n",
    "\n",
    "    sns.set(style='darkgrid')\n",
    "    sns.lineplot(x=names, y='score', data=df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вычислим значения `accuracy` для моделей с разным числом соседей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from tqdm import tqdm\n",
    "\n",
    "best_n = -1\n",
    "best_score = -1\n",
    "scores = []\n",
    "for n in tqdm(ns):\n",
    "    knn = KNeighborsClassifier(p=1, n_neighbors=n)\n",
    "    knn.fit(X_train, y_train)\n",
    "    y_pred = knn.predict(X_test)\n",
    "    score = knn.score(X_test, y_test)\n",
    "    \n",
    "    if score > best_score:\n",
    "        best_score = score\n",
    "        best_n = n\n",
    "    scores.append(score)\n",
    "\n",
    "plot_score(ns, scores, 'neighbors')\n",
    "print(f'Лучшая модель: {best_n} соседей, точность: {best_score}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Конечно, такой метод оценки качества модели не самый надежный, лучше воспользоваться\n",
    "оценкой методом кросс-валидации — `cross_val_score`.\n",
    "\n",
    "В дальнейшем будем использовать именно этот метод."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "best_n = -1\n",
    "best_score = -1\n",
    "scores = []\n",
    "for n in tqdm(ns):\n",
    "    knn = KNeighborsClassifier(p=2, n_neighbors=n)\n",
    "    \n",
    "    score = cross_val_score(knn, x, y, cv=5).mean()\n",
    "    \n",
    "    if score > best_score:\n",
    "        best_score = score\n",
    "        best_n = n\n",
    "    scores.append(score)\n",
    "\n",
    "plot_score(ns, scores, 'neighbors')\n",
    "print(f'Лучшая модель: {best_n} соседей, точность: {best_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label2vec(n_classes):\n",
    "  def inner(label):  \n",
    "    v = np.zeros(n_classes)\n",
    "    v[label - 1] = 1\n",
    "    return v\n",
    "  return inner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "\n",
    "\n",
    "scores = []\n",
    "for n in tqdm(ns):\n",
    "    knn = KNeighborsClassifier(p=2, n_neighbors=n)\n",
    "    knn.fit(X_train, y_train)\n",
    "    probs = knn.predict_proba(X_test)\n",
    "    \n",
    "    loss = log_loss(y_test, probs)\n",
    "    scores.append(loss)\n",
    "\n",
    "plot_score(ns, scores, 'neighbors')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVC"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C-svc"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### форма функции решения: one vs one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "\n",
    "scores = []\n",
    "ps = list(map(int, range(1, 10, 1)))\n",
    "for p in tqdm(ps):\n",
    "    svc = svm.SVC(degree=p, decision_function_shape='ovo')\n",
    "    svc.fit(X_train, y_train)\n",
    "    scores.append(svc.score(X_test, y_test))\n",
    "\n",
    "plot_score(ps, scores, 'degree')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "Cs = list(map(int, range(1, 100, 1)))\n",
    "\n",
    "for c in tqdm(Cs):\n",
    "    svc = svm.SVC(C=c, decision_function_shape='ovo')\n",
    "    svc.fit(X_train, y_train)\n",
    "    scores.append(svc.score(X_test, y_test))\n",
    "\n",
    "plot_score(Cs, scores, 'C-argument')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores =[]\n",
    "for c in tqdm(Cs):\n",
    "    svc = svm.SVC(C=c, decision_function_shape='ovo')\n",
    "    cvs = cross_val_score(svc, x, y, cv=5)\n",
    "    scores.append(cvs.mean())\n",
    "\n",
    "plot_score(Cs, scores, 'C-argument')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.multiclass import OneVsOneClassifier\n",
    "\n",
    "\n",
    "scores = []\n",
    "Cs = list(map(int, range(1, 100, 1)))\n",
    "\n",
    "for c in tqdm(Cs):\n",
    "    clf = OneVsOneClassifier(svm.SVC(C=c))\n",
    "    clf.fit(X_train, y_train)\n",
    "    scores.append(clf.score(X_test, y_test))\n",
    "\n",
    "plot_score(Cs, scores, 'C-argument')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "\n",
    "for c in tqdm(Cs):\n",
    "    clf = OneVsOneClassifier(svm.SVC(C=c))\n",
    "    cvs = cross_val_score(clf, x, y, cv=5)\n",
    "    scores.append(cvs.mean())\n",
    "plot_score(Cs, scores, 'C-argument')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### форма функции решения: one vs rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "\n",
    "for c in tqdm(Cs):\n",
    "    svc = svm.SVC(C=c, break_ties=True)\n",
    "    svc.fit(X_train, y_train)\n",
    "    scores.append(svc.score(X_test, y_test))\n",
    "\n",
    "plot_score(Cs, scores, 'C-argument')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "\n",
    "scores = []\n",
    "for c in tqdm(Cs):\n",
    "    clf = OneVsRestClassifier(svm.SVC(C=c))\n",
    "    clf.fit(X_train, y_train)\n",
    "    scores.append(clf.score(X_test, y_test))\n",
    "\n",
    "plot_score(Cs, scores, 'C-argument')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "Cs = list(map(int, range(1, 50, 1)))\n",
    "for c in tqdm(Cs):\n",
    "    clf = OneVsRestClassifier(svm.SVC(C=c))\n",
    "    cvs = cross_val_score(clf, x, y, cv=5)\n",
    "    scores.append(cvs.mean())\n",
    "\n",
    "plot_score(Cs, scores, 'C-argument')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\nu$-svc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []\n",
    "Nus = np.arange(0.01, 0.5, 0.01)\n",
    "for nu in tqdm(Nus):\n",
    "    nu_svc = svm.NuSVC(nu=nu, decision_function_shape='ovo')\n",
    "    nu_svc.fit(X_train, y_train)\n",
    "    scores.append(nu_svc.score(X_test, y_test))\n",
    "\n",
    "plot_score(Nus, scores, 'Nu-argument')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Нейронные сети"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Возьмем в качестве модели многослойный перцептрон, для борьбы с переобучением\n",
    "воспользуемся слоями `Dropout`. Кроме того, добавим между слоями\n",
    "нормализацию по подвыборке для того, чтобы сгладить процесс обучения.\n",
    "\n",
    "В качестве функции активации выберем `leaky_relu` как функцию, которая в отличие\n",
    "от `relu` не приводит к т.н. Dying RELU problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dropout, Dense, BatchNormalization\n",
    "from keras.backend import clear_session\n",
    "\n",
    "clear_session()\n",
    "\n",
    "lr = 0.001\n",
    "\n",
    "model = keras.Sequential()\n",
    "model.add(Input(X_train.shape[1]))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(.3))\n",
    "model.add(Dense(128, activation='leaky_relu'))\n",
    "model.add(Dropout(.2))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(32, activation='leaky_relu'))\n",
    "model.add(Dense(8, activation='softmax'))\n",
    "\n",
    "\n",
    "model.compile(optimizer=keras.optimizers.Adam(learning_rate=lr),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['categorical_accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.array(list(map(label2vec(n_classes), y_train)))\n",
    "y_test = np.array(list(map(label2vec(n_classes), y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import TensorBoard, ModelCheckpoint, CSVLogger\n",
    "\n",
    "callbacks = [\n",
    "  TensorBoard(),\n",
    "  ModelCheckpoint('checkpoint/', save_best_only=True, save_weights_only=True, monitor='categorical_accuracy'),\n",
    "  CSVLogger(\"logs.csv\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.optimizer.learning_rate = 0.00001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=200, callbacks=callbacks, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('checkpoint/')\n",
    "model.evaluate(X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "e64c9a8690db34c99b15a1f7c3e93a32cbaa4232add629869dab3afba3bd6272"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
